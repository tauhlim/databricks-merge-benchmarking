{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138507c8-95c1-4522-838b-47c2c2cc03da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('ServerlessJobID', 'MISSING', 'Serverless Job ID')\n",
    "dbutils.widgets.text('ClassicJobID', 'MISSING', 'Classic Job ID')\n",
    "\n",
    "ServerlessJobID = dbutils.widgets.get('ServerlessJobID')\n",
    "ClassicJobID = dbutils.widgets.get('ClassicJobID')\n",
    "\n",
    "assert ServerlessJobID != 'MISSING', \"Need to provide Job ID for Serverless Job\"\n",
    "assert ClassicJobID != 'MISSING', \"Need to provide Job ID for Classic Job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae8bdea-3ef6-4d5f-9a36-0cbba75f6008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_runs = spark.sql(f\"\"\"\n",
    "with latest_price as (\n",
    "  select\n",
    "    sku_name,\n",
    "    pricing.default as list_price,\n",
    "    row_number() over (partition by sku_name order by price_start_time desc) as rn\n",
    "  FROM\n",
    "    system.billing.list_prices\n",
    "),\n",
    "jobs_metadata AS (\n",
    "  select\n",
    "    job_id,\n",
    "    `name` as job_name,\n",
    "    row_number() over (partition by job_id order by change_time desc) as rn\n",
    "  from\n",
    "    system.lakeflow.jobs\n",
    "  WHERE\n",
    "    `job_id` IN ({ServerlessJobID}, {ClassicJobID})\n",
    "),\n",
    "job_run_metadata AS (\n",
    "  select\n",
    "    jrt.run_id,\n",
    "    jrt.period_start_time as period_start_time,\n",
    "    jrt.job_parameters,\n",
    "    row_number() over (partition by jrt.run_id order by period_start_time) as rn\n",
    "  from\n",
    "    system.lakeflow.job_run_timeline jrt\n",
    "  WHERE jrt.job_id IN ({ServerlessJobID}, {ClassicJobID})\n",
    "),\n",
    "latest_job_status AS (\n",
    "  select\n",
    "    jrt.run_id,\n",
    "    jrt.period_end_time as period_end_time,\n",
    "    jrt.termination_code,\n",
    "    row_number() over (partition by jrt.run_id order by period_start_time DESC) as rn\n",
    "  from\n",
    "    system.lakeflow.job_run_timeline jrt\n",
    "  WHERE jrt.job_id IN ({ServerlessJobID}, {ClassicJobID})\n",
    ")\n",
    "select\n",
    "  job_name,\n",
    "  sum(u.usage_quantity) as dbus_emitted,\n",
    "  u.usage_metadata.job_id,\n",
    "  u.usage_metadata.job_run_id,\n",
    "  CONVERT_TIMEZONE('UTC', 'Asia/Singapore', jrm.period_start_time) as job_run_start,\n",
    "  CONVERT_TIMEZONE('UTC', 'Asia/Singapore', latest_job_status.period_end_time) as job_run_end,\n",
    "  jrm.job_parameters,\n",
    "  latest_job_status.termination_code,\n",
    "  lp.list_price,\n",
    "  sum(u.usage_quantity * lp.list_price) as list_cost\n",
    "from\n",
    "  system.billing.usage u\n",
    "    left join latest_price lp\n",
    "      on u.sku_name = lp.sku_name\n",
    "      and lp.rn = 1\n",
    "    inner join jobs_metadata job\n",
    "      on u.usage_metadata.job_id = job.job_id\n",
    "      and job.rn = 1\n",
    "    inner join job_run_metadata jrm\n",
    "      on u.usage_metadata.job_run_id = jrm.run_id\n",
    "      and jrm.rn = 1\n",
    "    inner join latest_job_status\n",
    "      ON latest_job_status.run_id = jrm.run_id\n",
    "      and latest_job_status.rn = 1\n",
    "WHERE\n",
    "  `usage_date` >= '2025-08-18' \n",
    "GROUP BY\n",
    "  ALL\n",
    "ORDER BY\n",
    "  job_run_start;\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1185f989-39bf-4a94-aabb-e12c1359d680",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Azure VM List Pricing"
    }
   },
   "outputs": [],
   "source": [
    "values = [('Standard_D4ds_v5', 0.2260),('Standard_E8ads_v5', 0.5240)]\n",
    "columns = ['node_type', 'price_per_hour']\n",
    "df = spark.createDataFrame(values, columns)\n",
    "\n",
    "df.createOrReplaceTempView('node_prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1c3b8f-b8a5-40d7-8bd2-c0707e5d7614",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Cluster Costs (Estimated)"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH clusters as (\n",
    "  SELECT\n",
    "    get(regexp_extract_all(cluster_name, '\\\\\\\\d+', 0),0) as job_id,\n",
    "    get(regexp_extract_all(cluster_name, '\\\\\\\\d+', 0),1) as run_id,\n",
    "    cluster_id\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "  WHERE\n",
    "    cluster_name LIKE 'job-{ClassicJobID}%'\n",
    "),\n",
    "cluster_size_timeline AS (\n",
    "  SELECT DISTINCT\n",
    "    clusters.job_id,\n",
    "    clusters.run_id,\n",
    "    count(1) as num_nodes,\n",
    "    CONVERT_TIMEZONE('UTC', 'Asia/Singapore', nodes.start_time) as start_time,\n",
    "    CONVERT_TIMEZONE('UTC', 'Asia/Singapore', nodes.end_time) as end_time,\n",
    "    nodes.node_type\n",
    "  FROM\n",
    "    system.compute.node_timeline nodes\n",
    "      INNER JOIN clusters\n",
    "        ON nodes.cluster_id = clusters.cluster_id\n",
    "  GROUP BY\n",
    "    ALL\n",
    "),\n",
    "cluster_cost as (SELECT\n",
    "  clusters.*, date_diff(MINUTE, clusters.start_time, clusters.end_time)/60.0 as duration,\n",
    "  node_prices.price_per_hour, duration * clusters.num_nodes * node_prices.price_per_hour as cluster_cost\n",
    "FROM\n",
    "  cluster_size_timeline clusters\n",
    "    left join node_prices\n",
    "      on clusters.node_type = node_prices.node_type)\n",
    "\n",
    "SELECT job_id, run_id, min(start_time) as start_time, max(end_time) as end_time, sum(cluster_cost) as cluster_cost_dollars FROM cluster_cost\n",
    "GROUP BY job_id, run_id;\n",
    "\"\"\"\n",
    "\n",
    "compute_cluster_costs = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75091e35-3127-4dba-9099-71ac43a04e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Get total costs"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT \n",
    "    job_name, \n",
    "    job_runs.job_id, \n",
    "    job_runs.job_run_id, \n",
    "    job_run_start, \n",
    "    job_run_end, \n",
    "    dbus_emitted, \n",
    "    list_cost as list_cost_dbus,\n",
    "    coalesce(cluster_costs.cluster_cost_dollars,0) as list_cost_vms,\n",
    "    list_cost_dbus + list_cost_vms as total_cost,\n",
    "    job_runs.job_parameters\n",
    "    from {benchmark_job_runs} job_runs \n",
    "    left join {compute_cluster_costs} cluster_costs\n",
    "    on job_runs.job_id = cluster_costs.job_id\n",
    "    and job_runs.job_run_id = cluster_costs.run_id\n",
    "    WHERE termination_code = 'SUCCESS'\"\"\",\n",
    "    benchmark_job_runs=job_runs,\n",
    "    compute_cluster_costs=compute_cluster_costs,\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded5fbb3-eb68-44e2-8f43-6fd128908b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_GetSystemTablesCost",
   "widgets": {
    "ClassicJobID": {
     "currentValue": "52029122508262",
     "nuid": "5107255e-8ad4-4a1c-a1c0-adb9d5ac915d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "MISSING",
      "label": "Classic Job ID",
      "name": "ClassicJobID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "MISSING",
      "label": "Classic Job ID",
      "name": "ClassicJobID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "ServerlessJobID": {
     "currentValue": "180309930651617",
     "nuid": "a583549c-a938-4dbf-9ce5-38dd1f49c932",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "MISSING",
      "label": "Serverless Job ID",
      "name": "ServerlessJobID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "MISSING",
      "label": "Serverless Job ID",
      "name": "ServerlessJobID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
